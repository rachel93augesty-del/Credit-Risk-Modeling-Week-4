{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e36dba2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\venv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:177: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance.\n",
      "  return FileStore(store_uri, store_uri)\n",
      "D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\venv\\Lib\\site-packages\\scorecardpy\\germancredit.py:5: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: file:///D:/Personal/KAIM-10%20Academy/Week%204/Project/Credit-Risk-Modeling-Week%204/mlruns\n",
      "MLflow experiment set successfully!\n",
      "Data path set to: D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\data\\processed\\rfm_model_ready.csv\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Imports and setup\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "# Project root for module imports\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "\n",
    "# -----------------------------\n",
    "# MLflow setup (Windows-safe)\n",
    "# -----------------------------\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Use local mlruns folder\n",
    "mlruns_path = Path(project_root) / \"mlruns\"\n",
    "mlflow_tracking_uri = mlruns_path.as_uri()  # Converts to file:///D:/... format correctly\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "# Create or set experiment (tracking only; registry won't work locally)\n",
    "mlflow.set_experiment(\"Credit_Risk_Modeling_Task5\")\n",
    "\n",
    "print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())\n",
    "print(\"MLflow experiment set successfully!\")\n",
    "\n",
    "# -----------------------------\n",
    "# Module imports from src\n",
    "# -----------------------------\n",
    "from src.data_processing import (\n",
    "    load_data,\n",
    "    get_numeric_columns,\n",
    "    get_categorical_columns,\n",
    "    data_overview,\n",
    "    summarize_numeric,\n",
    "    summarize_categorical,\n",
    "    missing_value_report,\n",
    "    plot_correlation_matrix,\n",
    "    top_correlations,\n",
    "    plot_numeric_histograms,\n",
    "    plot_numeric_density,\n",
    "    plot_numeric_boxplots,\n",
    "    plot_categorical_distribution,\n",
    "    detect_outliers\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Data path for processed dataset\n",
    "# -----------------------------\n",
    "data_path = Path(project_root) / \"data\" / \"processed\" / \"rfm_model_ready.csv\"\n",
    "print(\"Data path set to:\", data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee2d81-f3c1-4c78-9207-2ab69614a819",
   "metadata": {},
   "source": [
    "Data Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25eaa4a5-f29d-4738-939e-0bc0535ad118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Data Loaded ‚úÖ\n",
      "Shape: (3632, 7)\n",
      "Step 2: Features and target separated ‚úÖ\n",
      "X shape: (3632, 6), y shape: (3632,)\n",
      "Step 3: Train/Test split ‚úÖ\n",
      "Train: (2905, 6), Test: (727, 6)\n",
      "Step 4: Features identified ‚úÖ\n",
      "Numerical: ['Recency', 'Frequency', 'Monetary']\n",
      "Categorical: ['Cluster']\n",
      "Step 5: Preprocessor created ‚úÖ\n",
      "Step 6: Preprocessing applied ‚úÖ\n",
      "X_train processed shape: (2905, 5)\n",
      "X_test processed shape:  (727, 5)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TASK 5.2: DATA PREPARATION - Train/Test Split\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ Define project root and data path\n",
    "# -------------------------------\n",
    "project_root = os.path.abspath(\"..\")  # notebooks/ -> project root\n",
    "data_file = os.path.join(project_root, \"data\", \"processed\", \"rfm_model_ready.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ Load processed CSV\n",
    "# -------------------------------\n",
    "df = pd.read_csv(data_file)\n",
    "print(f\"Step 1: Data Loaded ‚úÖ\\nShape: {df.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Separate features and target\n",
    "# -------------------------------\n",
    "target_col = 'is_high_risk'\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "print(f\"Step 2: Features and target separated ‚úÖ\\nX shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ Split into train/test sets\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Step 3: Train/Test split ‚úÖ\\nTrain: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ Define numeric & categorical features\n",
    "# -------------------------------\n",
    "num_features = ['Recency', 'Frequency', 'Monetary']\n",
    "cat_features = ['Cluster']\n",
    "\n",
    "print(f\"Step 4: Features identified ‚úÖ\")\n",
    "print(f\"Numerical: {num_features}\")\n",
    "print(f\"Categorical: {cat_features}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ Create preprocessor\n",
    "# -------------------------------\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_features)\n",
    "    ]\n",
    ")\n",
    "print(\"Step 5: Preprocessor created ‚úÖ\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ Apply preprocessing\n",
    "# -------------------------------\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Step 6: Preprocessing applied ‚úÖ\")\n",
    "print(f\"X_train processed shape: {X_train_processed.shape}\")\n",
    "print(f\"X_test processed shape:  {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cfe77d-684c-48fd-98c1-e813e578f331",
   "metadata": {},
   "source": [
    "TASK 5.2: SETUP FOR REPRODUCIBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66af6e2f-a055-4d4a-aecb-00a172fb82df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Random State set to: 42 (for reproducibility)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TASK 5.2: SETUP FOR REPRODUCIBILITY\n",
    "# ============================================\n",
    "\n",
    "# GLOBAL RANDOM STATE - Set once, use everywhere\n",
    "RANDOM_STATE = 42  # Task 5.2 requirement: \"Ensure reproducibility by setting a random_state\"\n",
    "\n",
    "print(f\"üîß Random State set to: {RANDOM_STATE} (for reproducibility)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example usage in train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Example usage in models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8436067-a76c-49d5-be52-a947331e49d9",
   "metadata": {},
   "source": [
    " TASK 5.3: MODEL SELECTION AND TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29452b4d-1962-4c29-bb4b-2d4e189f9c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK 5.3: Training ALL 4 Models with Pipelines + Evaluation\n",
      "============================================================\n",
      "\n",
      "=== Logistic Regression ===\n",
      "Testing Accuracy: 0.623\n",
      "ROC-AUC: 0.7254067254067255\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.867     0.489     0.626       468\n",
      "           1      0.484     0.865     0.620       259\n",
      "\n",
      "    accuracy                          0.623       727\n",
      "   macro avg      0.676     0.677     0.623       727\n",
      "weighted avg      0.731     0.623     0.624       727\n",
      "\n",
      "\n",
      "=== Decision Tree ===\n",
      "Testing Accuracy: 0.685\n",
      "ROC-AUC: 0.7512952512952512\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.846     0.624     0.718       468\n",
      "           1      0.539     0.795     0.643       259\n",
      "\n",
      "    accuracy                          0.685       727\n",
      "   macro avg      0.693     0.710     0.681       727\n",
      "weighted avg      0.737     0.685     0.691       727\n",
      "\n",
      "\n",
      "=== Random Forest ===\n",
      "Testing Accuracy: 0.671\n",
      "ROC-AUC: 0.7401742401742402\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.819     0.628     0.711       468\n",
      "           1      0.527     0.749     0.619       259\n",
      "\n",
      "    accuracy                          0.671       727\n",
      "   macro avg      0.673     0.689     0.665       727\n",
      "weighted avg      0.715     0.671     0.678       727\n",
      "\n",
      "\n",
      "=== Gradient Boosting ===\n",
      "Testing Accuracy: 0.718\n",
      "ROC-AUC: 0.7790441540441541\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.845     0.688     0.759       468\n",
      "           1      0.578     0.772     0.661       259\n",
      "\n",
      "    accuracy                          0.718       727\n",
      "   macro avg      0.712     0.730     0.710       727\n",
      "weighted avg      0.750     0.718     0.724       727\n",
      "\n",
      "\n",
      "‚úÖ Task 5.3 COMPLETE: Trained 4 models with pipelines\n",
      "   ‚úì Logistic Regression ‚úì Decision Tree ‚úì Random Forest ‚úì Gradient Boosting\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TASK 5.3: TRAIN ALL 4 MODELS WITH PIPELINES + EVALUATION\n",
    "# ============================================\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(\"TASK 5.3: Training ALL 4 Models with Pipelines + Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Preprocessing pipeline\n",
    "# ---------------------------\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), ['Frequency', 'Monetary'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Model pipelines\n",
    "# ---------------------------\n",
    "pipelines = {\n",
    "    'Logistic Regression': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(\n",
    "            random_state=42, max_iter=1000, class_weight='balanced'\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'Decision Tree': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', DecisionTreeClassifier(\n",
    "            random_state=42, class_weight='balanced', max_depth=5\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'Random Forest': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            random_state=42, n_estimators=100, class_weight='balanced'\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'Gradient Boosting': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', GradientBoostingClassifier(\n",
    "            random_state=42, n_estimators=100, learning_rate=0.1\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Train & Evaluate\n",
    "# ---------------------------\n",
    "trained_pipelines = {}\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    trained_pipelines[name] = pipeline\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline.named_steps['classifier'], \"predict_proba\") else None\n",
    "    \n",
    "    # Metrics\n",
    "    test_acc = pipeline.score(X_test, y_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else \"N/A\"\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Testing Accuracy: {test_acc:.3f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(f\"\\n‚úÖ Task 5.3 COMPLETE: Trained {len(trained_pipelines)} models with pipelines\")\n",
    "print(\"   ‚úì Logistic Regression ‚úì Decision Tree ‚úì Random Forest ‚úì Gradient Boosting\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ee293-af38-4ffe-9c23-dc9a98487440",
   "metadata": {},
   "source": [
    "Setup MLflow Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "493a1360-7527-41c8-ab8f-49f747749c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow experiment ready: Credit_Risk_Modeling_Task5\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "EXPERIMENT_NAME = \"Credit_Risk_Modeling_Task5\"\n",
    "\n",
    "mlruns_path = \"./mlruns\"  # relative path (no file:// needed)\n",
    "mlflow.set_tracking_uri(mlruns_path)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"MLflow experiment ready: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f89a973d-4044-4c68-9daf-c68d2dc627dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression ===\n",
      "Testing Accuracy: 0.644\n",
      "ROC-AUC: 0.512\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78       468\n",
      "           1       0.00      0.00      0.00       259\n",
      "\n",
      "    accuracy                           0.64       727\n",
      "   macro avg       0.32      0.50      0.39       727\n",
      "weighted avg       0.41      0.64      0.50       727\n",
      "\n",
      "\n",
      "=== Decision Tree ===\n",
      "Testing Accuracy: 0.536\n",
      "ROC-AUC: 0.487\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.66      0.65       468\n",
      "           1       0.33      0.31      0.32       259\n",
      "\n",
      "    accuracy                           0.54       727\n",
      "   macro avg       0.48      0.48      0.48       727\n",
      "weighted avg       0.53      0.54      0.53       727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest ===\n",
      "Testing Accuracy: 0.568\n",
      "ROC-AUC: 0.500\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.72      0.68       468\n",
      "           1       0.37      0.30      0.33       259\n",
      "\n",
      "    accuracy                           0.57       727\n",
      "   macro avg       0.51      0.51      0.51       727\n",
      "weighted avg       0.55      0.57      0.56       727\n",
      "\n",
      "\n",
      "=== Gradient Boosting ===\n",
      "Testing Accuracy: 0.620\n",
      "ROC-AUC: 0.510\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.94      0.76       468\n",
      "           1       0.28      0.04      0.07       259\n",
      "\n",
      "    accuracy                           0.62       727\n",
      "   macro avg       0.46      0.49      0.42       727\n",
      "weighted avg       0.51      0.62      0.52       727\n",
      "\n",
      "\n",
      "‚úÖ Task 5.3 COMPLETE: Trained 4 models with pipelines\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TASK 5.3: Training ALL 4 Models with Pipelines + Evaluation\n",
    "# ============================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    roc = roc_auc_score(y_test, model.predict_proba(X_test_processed)[:,1])\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Testing Accuracy: {acc:.3f}\")\n",
    "    print(f\"ROC-AUC: {roc:.3f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "print(\"\\n‚úÖ Task 5.3 COMPLETE: Trained 4 models with pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f320ec-206c-4145-8763-e8b2d0a0f113",
   "metadata": {},
   "source": [
    "TASK 5.3: MLflow Logging of Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3058b43b-412b-489d-859c-80eb666b1ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged Logistic Regression to MLflow with Accuracy=0.644, ROC-AUC=0.512\n",
      "‚úÖ Logged Decision Tree to MLflow with Accuracy=0.536, ROC-AUC=0.487\n",
      "‚úÖ Logged Random Forest to MLflow with Accuracy=0.568, ROC-AUC=0.500\n",
      "‚úÖ Logged Gradient Boosting to MLflow with Accuracy=0.620, ROC-AUC=0.510\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TASK 5.3: MLflow Logging of Trained Models\n",
    "# ============================================\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "EXPERIMENT_NAME = \"Credit_Risk_Modeling_Task5\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, name=\"model_pipeline\")\n",
    "        \n",
    "        # Predict on test set\n",
    "        y_pred = model.predict(X_test_processed)\n",
    "        y_proba = model.predict_proba(X_test_processed)[:,1] if hasattr(model, \"predict_proba\") else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        if roc is not None:\n",
    "            mlflow.log_metric(\"roc_auc\", roc)\n",
    "        \n",
    "        # Print summary safely\n",
    "        roc_str = f\"{roc:.3f}\" if roc is not None else \"N/A\"\n",
    "        print(f\"‚úÖ Logged {name} to MLflow with Accuracy={acc:.3f}, ROC-AUC={roc_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0509fdf-0a46-489c-bbcd-52b040830531",
   "metadata": {},
   "source": [
    "Task 5.4 ‚Äì Hyperparameter Tuning for Credit Risk Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9322c77e-5fe2-4e05-b069-9d29eac0b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/18 09:15:18 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "2025/12/18 09:15:22 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged Logistic Regression to MLflow with Accuracy=0.644, ROC-AUC=0.512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78       468\n",
      "           1       0.00      0.00      0.00       259\n",
      "\n",
      "    accuracy                           0.64       727\n",
      "   macro avg       0.32      0.50      0.39       727\n",
      "weighted avg       0.41      0.64      0.50       727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/18 09:15:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged Decision Tree to MLflow with Accuracy=0.536, ROC-AUC=0.487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.66      0.65       468\n",
      "           1       0.33      0.31      0.32       259\n",
      "\n",
      "    accuracy                           0.54       727\n",
      "   macro avg       0.48      0.48      0.48       727\n",
      "weighted avg       0.53      0.54      0.53       727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/18 09:15:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged Random Forest to MLflow with Accuracy=0.568, ROC-AUC=0.500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.72      0.68       468\n",
      "           1       0.37      0.30      0.33       259\n",
      "\n",
      "    accuracy                           0.57       727\n",
      "   macro avg       0.51      0.51      0.51       727\n",
      "weighted avg       0.55      0.57      0.56       727\n",
      "\n",
      "‚úÖ Logged Gradient Boosting to MLflow with Accuracy=0.620, ROC-AUC=0.510\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.94      0.76       468\n",
      "           1       0.28      0.04      0.07       259\n",
      "\n",
      "    accuracy                           0.62       727\n",
      "   macro avg       0.46      0.49      0.42       727\n",
      "weighted avg       0.51      0.62      0.52       727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TASK 5.4: Hyperparameter Tuning + MLflow Logging\n",
    "# ============================================\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "EXPERIMENT_NAME = \"Credit_Risk_Modeling_Task5\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Assume `models` is a dictionary of your trained/tuned models\n",
    "# e.g., models = {'Logistic Regression': tuned_lr, ...}\n",
    "\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, artifact_path=\"model_pipeline\")\n",
    "        \n",
    "        # Predict on test set\n",
    "        y_pred = model.predict(X_test_processed)\n",
    "        y_proba = model.predict_proba(X_test_processed)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        if roc is not None:\n",
    "            mlflow.log_metric(\"roc_auc\", roc)\n",
    "        \n",
    "        # Safely format ROC-AUC for printing\n",
    "        roc_str = f\"{roc:.3f}\" if roc is not None else \"N/A\"\n",
    "        print(f\"‚úÖ Logged {name} to MLflow with Accuracy={acc:.3f}, ROC-AUC={roc_str}\")\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd8676-16b9-47db-8afc-5a83da4d35fb",
   "metadata": {},
   "source": [
    "Save the best models from Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c824a09-9bbf-470e-89a1-217de0502dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved Gradient_Boosting model at D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\saved_models\\Gradient_Boosting_best_model.pkl\n",
      "‚úÖ Saved Random_Forest model at D:\\Personal\\KAIM-10 Academy\\Week 4\\Project\\Credit-Risk-Modeling-Week 4\\saved_models\\Random_Forest_best_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TASK 5.5: Save Best Models Locally\n",
    "# ============================================\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create a folder to save the models\n",
    "models_dir = os.path.join(project_root, \"saved_models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the best models from GridSearchCV\n",
    "best_models = {\n",
    "    \"Gradient_Boosting\": tuned_gb_model,\n",
    "    \"Random_Forest\": tuned_rf_model\n",
    "}\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    file_path = os.path.join(models_dir, f\"{name}_best_model.pkl\")\n",
    "    joblib.dump(model, file_path)\n",
    "    print(f\"‚úÖ Saved {name} model at {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12daa87b-984d-497f-aace-892599a9e781",
   "metadata": {},
   "source": [
    " TASK 5.5: EXPERIMENT TRACKING AND MODEL REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0baf8d7f-29d1-420a-a582-c4c7d86a7f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient_Boosting Test Accuracy: 0.613, ROC-AUC: 0.510\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.91      0.75       468\n",
      "           1       0.32      0.07      0.12       259\n",
      "\n",
      "    accuracy                           0.61       727\n",
      "   macro avg       0.48      0.49      0.44       727\n",
      "weighted avg       0.52      0.61      0.53       727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Best_Gradient_Boosting_CreditRisk' already exists. Creating a new version of this model...\n",
      "2025/12/18 09:18:25 WARNING mlflow.tracking._model_registry.fluent: Run with id 5a7d8400b0db4d9ca67ba8ab50504241 has no artifacts at artifact path 'model_pipeline', registering model based on models:/m-3d0ad266ee414b7ab166444002d57bb8 instead\n",
      "Created version '3' of model 'Best_Gradient_Boosting_CreditRisk'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered Gradient_Boosting model in MLflow Model Registry\n",
      "\n",
      "\n",
      "Random_Forest Test Accuracy: 0.549, ROC-AUC: 0.505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.64      0.65       468\n",
      "           1       0.37      0.38      0.37       259\n",
      "\n",
      "    accuracy                           0.55       727\n",
      "   macro avg       0.51      0.51      0.51       727\n",
      "weighted avg       0.55      0.55      0.55       727\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'Best_Random_Forest_CreditRisk'.\n",
      "2025/12/18 09:18:30 WARNING mlflow.tracking._model_registry.fluent: Run with id a9ac998ee0f14db2924c80c12467bcfe has no artifacts at artifact path 'model_pipeline', registering model based on models:/m-d9c38c0eacec4c00a2d8cb05ea0ba8e7 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered Random_Forest model in MLflow Model Registry\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'Best_Random_Forest_CreditRisk'.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TASK 5.5: EXPERIMENT TRACKING AND MODEL REGISTRY\n",
    "# ============================================\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "EXPERIMENT_NAME = \"Credit_Risk_Modeling_Task5\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Best models from hyperparameter tuning\n",
    "best_models = {\n",
    "    \"Gradient_Boosting\": tuned_gb_model,\n",
    "    \"Random_Forest\": tuned_rf_model\n",
    "}\n",
    "for name, model in best_models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        # Log the model in MLflow\n",
    "        mlflow.sklearn.log_model(model, name=\"model_pipeline\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = model.predict(X_test_processed)\n",
    "        y_proba = model.predict_proba(X_test_processed)[:,1] if hasattr(model, \"predict_proba\") else None\n",
    "        \n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        if roc is not None:\n",
    "            mlflow.log_metric(\"roc_auc\", roc)\n",
    "        \n",
    "        # Print classification report safely\n",
    "        roc_display = f\"{roc:.3f}\" if roc is not None else \"N/A\"\n",
    "        print(f\"\\n{name} Test Accuracy: {acc:.3f}, ROC-AUC: {roc_display}\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        \n",
    "        # Register the model to MLflow Model Registry\n",
    "        model_uri = f\"runs:/{mlflow.active_run().info.run_id}/model_pipeline\"\n",
    "        mlflow.register_model(model_uri, f\"Best_{name}_CreditRisk\")\n",
    "        print(f\"‚úÖ Registered {name} model in MLflow Model Registry\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79286133-f5c0-42d2-a2f4-c7d556dcb474",
   "metadata": {},
   "source": [
    "Task 5.6 ‚Äì Register the best model in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adc79c3f-4225-419b-809c-94d3b035dfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/18 09:21:13 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/12/18 09:21:17 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged pipeline Gradient_Boosting to MLflow\n",
      "‚úÖ Logged pipeline Random_Forest to MLflow\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# Example: Wrap your model in a pipeline with preprocessing\n",
    "gb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),       # the ColumnTransformer\n",
    "    ('classifier', GradientBoostingClassifier(\n",
    "        learning_rate=0.1, max_depth=3, \n",
    "        n_estimators=150, subsample=1.0,\n",
    "        random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        max_depth=10, min_samples_split=2, \n",
    "        min_samples_leaf=1, n_estimators=100, \n",
    "        random_state=42))\n",
    "])\n",
    "\n",
    "# Train pipelines on training data\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Log the full pipelines to MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "EXPERIMENT_NAME = \"Credit_Risk_Modeling_Task5\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "pipelines = {\"Gradient_Boosting\": gb_pipeline, \"Random_Forest\": rf_pipeline}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        mlflow.sklearn.log_model(pipe, \"model_pipeline\")\n",
    "        print(f\"‚úÖ Logged pipeline {name} to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a709d02-5878-4f3a-87df-9244b8483f8c",
   "metadata": {},
   "source": [
    "TASK 5.6: Model Evaluation for Registered Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e5472a1-0d5f-41ef-8a56-da2366f63dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/18 09:27:16 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/12/18 09:27:20 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged full pipeline Gradient_Boosting to MLflow\n",
      "‚úÖ Logged full pipeline Random_Forest to MLflow\n",
      "\n",
      "Gradient_Boosting Test Accuracy: 0.715, ROC-AUC: 0.781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.70      0.76       468\n",
      "           1       0.58      0.74      0.65       259\n",
      "\n",
      "    accuracy                           0.72       727\n",
      "   macro avg       0.70      0.72      0.70       727\n",
      "weighted avg       0.74      0.72      0.72       727\n",
      "\n",
      "‚úÖ Logged evaluation metrics for Gradient_Boosting to MLflow\n",
      "\n",
      "Random_Forest Test Accuracy: 0.691, ROC-AUC: 0.762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       468\n",
      "           1       0.55      0.72      0.62       259\n",
      "\n",
      "    accuracy                           0.69       727\n",
      "   macro avg       0.68      0.70      0.68       727\n",
      "weighted avg       0.72      0.69      0.70       727\n",
      "\n",
      "‚úÖ Logged evaluation metrics for Random_Forest to MLflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Best_Gradient_Boosting_CreditRisk' already exists. Creating a new version of this model...\n",
      "2025/12/18 09:27:26 WARNING mlflow.tracking._model_registry.fluent: Run with id 1ab9f318dbaf4d0cb4e1ddc4549b52fa has no artifacts at artifact path 'model_pipeline', registering model based on models:/m-1aae25f60efd457fbe90228906cba74a instead\n",
      "Created version '5' of model 'Best_Gradient_Boosting_CreditRisk'.\n",
      "Registered model 'Best_Random_Forest_CreditRisk' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered Gradient_Boosting model in MLflow Model Registry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/18 09:27:27 WARNING mlflow.tracking._model_registry.fluent: Run with id 5d01fbe11b904ba999418e453414af6a has no artifacts at artifact path 'model_pipeline', registering model based on models:/m-6dd8376f26e840c0ab3ebbbf3c9112dc instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Registered Random_Forest model in MLflow Model Registry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'Best_Random_Forest_CreditRisk'.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TASK 5.6: Save, Evaluate, and Register Full Pipelines\n",
    "# ============================================\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# --- Best full pipelines (preprocessor + model)\n",
    "best_pipelines = {\n",
    "    \"Gradient_Boosting\": gb_pipeline,  # include preprocessing\n",
    "    \"Random_Forest\": rf_pipeline\n",
    "}\n",
    "\n",
    "EXPERIMENT_NAME = \"Credit_Risk_Modeling_Task5\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Log full pipelines to MLflow\n",
    "# ---------------------------\n",
    "logged_runs = {}  # store run_id for later\n",
    "for name, pipeline in best_pipelines.items():\n",
    "    with mlflow.start_run(run_name=name) as run:\n",
    "        mlflow.sklearn.log_model(pipeline, \"model_pipeline\")  # full pipeline\n",
    "        print(f\"‚úÖ Logged full pipeline {name} to MLflow\")\n",
    "        logged_runs[name] = run.info.run_id\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Evaluate the pipelines (using raw X_test)\n",
    "# ---------------------------\n",
    "for name, run_id in logged_runs.items():\n",
    "    model_uri = f\"runs:/{run_id}/model_pipeline\"\n",
    "    loaded_pipeline = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "    # Predict on raw test set (pipeline handles preprocessing)\n",
    "    y_pred = loaded_pipeline.predict(X_test)\n",
    "    y_proba = loaded_pipeline.predict_proba(X_test)[:, 1] if hasattr(loaded_pipeline, \"predict_proba\") else None\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "    roc_text = f\"{roc:.3f}\" if roc is not None else \"N/A\"\n",
    "\n",
    "    print(f\"\\n{name} Test Accuracy: {acc:.3f}, ROC-AUC: {roc_text}\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # Log evaluation metrics back to MLflow\n",
    "    with mlflow.start_run(run_name=f\"Evaluation_{name}\"):\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        if roc is not None:\n",
    "            mlflow.log_metric(\"roc_auc\", roc)\n",
    "        print(f\"‚úÖ Logged evaluation metrics for {name} to MLflow\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Register the best models\n",
    "# ---------------------------\n",
    "for name, run_id in logged_runs.items():\n",
    "    model_uri = f\"runs:/{run_id}/model_pipeline\"\n",
    "    model_registry_name = f\"Best_{name}_CreditRisk\"\n",
    "\n",
    "    try:\n",
    "        mlflow.register_model(model_uri, model_registry_name)\n",
    "        print(f\"‚úÖ Registered {name} model in MLflow Model Registry\")\n",
    "    except mlflow.exceptions.MlflowException as e:\n",
    "        print(f\"‚ö†Ô∏è Model {name} may already exist. Created new version. Details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748d971-adc5-4d27-a8e9-cbd5c8d14ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
